# bot.py
import os
import random
import time
import logging
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import tweepy
from dotenv import load_dotenv

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã‚„ãƒ†ã‚¹ãƒˆæ™‚ã« .env ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼ˆActionsã§ã¯env varsã‚’ä½¿ã†ã®ã§ç„¡åŠ¹ã§ã‚‚OKï¼‰
load_dotenv()

# X / Twitter ã®ã‚­ãƒ¼ï¼ˆGitHub Actions ã§ã¯ secrets çµŒç”±ã§ç’°å¢ƒå¤‰æ•°ã«å…¥ã‚Šã¾ã™ï¼‰
API_KEY = os.getenv("API_KEY")
API_SECRET = os.getenv("API_SECRET")
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = os.getenv("ACCESS_TOKEN_SECRET")

# æŠ•ç¨¿å…ˆã‚µã‚¤ãƒˆï¼ˆã‚ãªãŸã®ã‚µã‚¤ãƒˆï¼‰
TARGET_SITE = "https://active-gyoseisyosi.com/"

# æŠ•ç¨¿ã‚¹ã‚¿ã‚¤ãƒ«ã®å‰²åˆï¼ˆB:ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼80% / A:ä¸å¯§20%ï¼‰
STYLE_B_WEIGHT = 0.8

# æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆBå¯„ã‚ŠãŒå¤šã‚ï¼‰
TEMPLATES_B = [
    "ã“ã®è¨˜äº‹ã€æ°—ã«ãªã£ãŸæ–¹å¤šã„ã¿ãŸã„ï¼ãœã²èª­ã‚“ã§ã¿ã¦ãã ã•ã„ğŸ‘‡\n{title}\n{url}",
    "æœ€è¿‘ã®ãŠã™ã™ã‚è¨˜äº‹ã§ã™âœ¨\n{title}\n{url}",
    "ãŠã€é¢ç™½ã„æ›´æ–°ãŒã‚ã‚Šã¾ã—ãŸï¼ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ğŸ‘‡\n{title}\n{url}",
    "ã“ã®è¨˜äº‹ã€ãŸã‚ã«ãªã‚Šã¾ã—ãŸï¼å…±æœ‰ã—ã¾ã™ğŸ“\n{title}\n{url}",
    "ä¹…ã—ã¶ã‚Šã«èª­ã¿è¿”ã—ãŸã‚‰è‰¯ã‹ã£ãŸã§ã™ã€‚è‰¯ã‘ã‚Œã°ã©ã†ãğŸ‘‡\n{title}\n{url}"
]
TEMPLATES_A = [
    "æ–°ã—ã„è¨˜äº‹ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ã”èˆˆå‘³ã‚ã‚Œã°ã”è¦§ãã ã•ã„ã€‚\n{title}\n{url}",
    "æœ€æ–°ã®æŠ•ç¨¿ã‚’ãŠçŸ¥ã‚‰ã›ã—ã¾ã™ã€‚\n{title}\n{url}"
]

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼šè¤‡æ•°ã®æ–¹æ³•ã§è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼ˆå …ç‰¢åŒ–ï¼‰
def fetch_article_list(base_url):
    logging.info("Fetching site: %s", base_url)
    try:
        r = requests.get(base_url, timeout=15)
        r.raise_for_status()
    except Exception as e:
        logging.error("Failed to fetch site: %s", e)
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    candidates = []

    # 1) HTML5 <article> ã‚¿ã‚°ã‚’æ¢ã™
    for art in soup.find_all("article"):
        a = art.find("a", href=True)
        if a:
            title = (a.get_text() or "").strip()
            link = a["href"]
            candidates.append((title, link))

    # 2) ã‚¯ãƒ©ã‚¹åã« 'post' 'entry' 'article' ã‚’å«ã‚€è¦ç´ ã‹ã‚‰æ¢ã™
    if not candidates:
        selectors = ["h2 a", ".post a", ".entry-title a", ".article a", ".post-title a"]
        for sel in selectors:
            for a in soup.select(sel):
                if a and a.get("href"):
                    title = (a.get_text() or "").strip()
                    link = a["href"]
                    candidates.append((title, link))

    # 3) ãƒªãƒ³ã‚¯ç¾¤ã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®æœ‰åŠ›å€™è£œã‚’æ‹¾ã†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith("/") or base_url in href:
                text = (a.get_text() or "").strip()
                if len(text) > 10 and len(text) < 200:  # ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿
                    link = href if href.startswith("http") else base_url.rstrip("/") + href
                    candidates.append((text, link))

    # æ­£è¦åŒ–ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ã«ã™ã‚‹ï¼‰
    normalized = []
    for title, link in candidates:
        if not link.startswith("http"):
            link = base_url.rstrip("/") + "/" + link.lstrip("/")
        normalized.append((title, link))

    # é‡è¤‡é™¤å»ï¼ˆURLå„ªå…ˆï¼‰
    seen = set()
    out = []
    for t, l in normalized:
        if l not in seen:
            seen.add(l)
            out.append((t, l))
    logging.info("Found %d candidate articles", len(out))
    return out

# æŠ•ç¨¿æ–‡ç”Ÿæˆï¼ˆäººé–“ã‚‰ã—ã•ã‚’æ¼”å‡ºï¼‰
def compose_text(title, url):
    # ã‚¹ã‚¿ã‚¤ãƒ«é¸æŠï¼ˆB 80% / A 20%ï¼‰
    if random.random() < STYLE_B_WEIGHT:
        template = random.choice(TEMPLATES_B)
    else:
        template = random.choice(TEMPLATES_A)

    # è»½ã„ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ ï¼šå‰ç½®ããƒ»çµµæ–‡å­—
    prefixes = ["", "ğŸ”” ", "â€»", "âœ¨ ", ""]
    suffixes = ["", " #ãŠçŸ¥ã‚‰ã›", " #è¡Œæ”¿æ›¸å£«", ""]

    text = f"{random.choice(prefixes)}{template.format(title=title, url=url)}{random.choice(suffixes)}"

    # æ–‡å­—æ•°èª¿æ•´ï¼ˆTwitter ã¯ 280 æ–‡å­—ï¼‰
    if len(text) > 270:
        text = text[:267] + "..."
    return text

# æŠ•ç¨¿å‡¦ç†ï¼ˆTweepy ã‚’ä½¿ã†ï¼‰
def post_to_twitter(text):
    logging.info("Posting to Twitter: %s", text[:80].replace("\n"," ") + ("..." if len(text)>80 else ""))
    auth = tweepy.OAuth1UserHandler(API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
    api = tweepy.API(auth, wait_on_rate_limit=True)
    try:
        api.update_status(status=text)
        logging.info("Posted successfully.")
    except Exception as e:
        logging.error("Failed to post: %s", e)

# ãƒ¡ã‚¤ãƒ³ã®æµã‚Œ
def main():
    articles = fetch_article_list(TARGET_SITE)
    if not articles:
        logging.error("No articles found; aborting.")
        return

    # æ–°ç€ï¼ˆæœ€åˆã®1ä»¶ï¼‰ã‚’å„ªå…ˆã—ã¦æŠ•ç¨¿ã™ã‚‹ç¢ºç‡ã‚’é«˜ã‚ã‚‹
    # ãŸã¨ãˆã°: 60%ã§æœ€æ–°ã€40%ã§éå»è¨˜äº‹ãƒ©ãƒ³ãƒ€ãƒ æ˜ã‚Šèµ·ã“ã—
    if random.random() < 0.6:
        chosen = articles[0]  # ãƒšãƒ¼ã‚¸å†…æœ€åˆã®å€™è£œã‚’ã€Œæœ€æ–°ã€ã¨ä»®å®š
        logging.info("Choosing latest article")
    else:
        chosen = random.choice(articles)
        logging.info("Choosing random past article")

    title, url = chosen

    # URLãŒè¨˜äº‹å†…ã‚¢ãƒ³ã‚«ãƒ¼ãªã‚‰ãã‚Œã„ã«æ•´å½¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    tweet = compose_text(title, url)

    # å®Ÿè¡Œç’°å¢ƒå¤‰æ•° TWITTER_DRY_RUN ãŒ set ã•ã‚Œã¦ã„ã‚‹ã¨æŠ•ç¨¿ã—ãªã„ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    if os.getenv("TWITTER_DRY_RUN") == "1":
        logging.info("DRY RUN: would post:\n%s", tweet)
    else:
        # æŠ•ç¨¿å‰ã®å°ã•ãªãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿï¼ˆäººé–“ã£ã½ã•ï¼‰
        delay = random.randint(5, 90)  # 5ï½90ç§’ãƒ©ãƒ³ãƒ€ãƒ å¾…ã¡
        logging.info("Sleeping %d seconds before posting (human-like delay)", delay)
        time.sleep(delay)
        post_to_twitter(tweet)

if __name__ == "__main__":
    main()
