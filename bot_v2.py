# bot_v2.py
import os
import random
import time
import logging
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import tweepy
from dotenv import load_dotenv

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã‚„ãƒ†ã‚¹ãƒˆæ™‚ã« .env ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
load_dotenv()

# X / Twitter ã®ã‚­ãƒ¼ï¼ˆGitHub Actions ã§ã¯ secrets çµŒç”±ã§ç’°å¢ƒå¤‰æ•°ã«å…¥ã‚Šã¾ã™ï¼‰
API_KEY = os.getenv("API_KEY")
API_SECRET = os.getenv("API_SECRET")
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = os.getenv("ACCESS_TOKEN_SECRET")
BEARER_TOKEN = os.getenv("BEARER_TOKEN")  # v2 APIç”¨

# æŠ•ç¨¿å…ˆã‚µã‚¤ãƒˆ
TARGET_SITE = "https://active-gyoseisyosi.com/"

# æŠ•ç¨¿ã‚¹ã‚¿ã‚¤ãƒ«ã®å‰²åˆï¼ˆB:ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼80% / A:ä¸å¯§20%ï¼‰
STYLE_B_WEIGHT = 0.8

# æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
TEMPLATES_B = [
    "ã“ã®è¨˜äº‹ã€æ°—ã«ãªã£ãŸæ–¹å¤šã„ã¿ãŸã„ï¼ãœã²èª­ã‚“ã§ã¿ã¦ãã ã•ã„ğŸ‘‡\n{title}\n{url}",
    "æœ€è¿‘ã®ãŠã™ã™ã‚è¨˜äº‹ã§ã™âœ¨\n{title}\n{url}",
    "ãŠã€é¢ç™½ã„æ›´æ–°ãŒã‚ã‚Šã¾ã—ãŸï¼ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ğŸ‘‡\n{title}\n{url}",
    "ã“ã®è¨˜äº‹ã€ãŸã‚ã«ãªã‚Šã¾ã—ãŸï¼å…±æœ‰ã—ã¾ã™ğŸ“\n{title}\n{url}",
    "ä¹…ã—ã¶ã‚Šã«èª­ã¿è¿”ã—ãŸã‚‰è‰¯ã‹ã£ãŸã§ã™ã€‚è‰¯ã‘ã‚Œã°ã©ã†ãğŸ‘‡\n{title}\n{url}"
]
TEMPLATES_A = [
    "æ–°ã—ã„è¨˜äº‹ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ã”èˆˆå‘³ã‚ã‚Œã°ã”è¦§ãã ã•ã„ã€‚\n{title}\n{url}",
    "æœ€æ–°ã®æŠ•ç¨¿ã‚’ãŠçŸ¥ã‚‰ã›ã—ã¾ã™ã€‚\n{title}\n{url}"
]

# v2 API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
client = tweepy.Client(
    consumer_key=API_KEY,
    consumer_secret=API_SECRET,
    access_token=ACCESS_TOKEN,
    access_token_secret=ACCESS_TOKEN_SECRET,
    bearer_token=BEARER_TOKEN
)

# æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ
def compose_text(title, url):
    if random.random() < STYLE_B_WEIGHT:
        template = random.choice(TEMPLATES_B)
    else:
        template = random.choice(TEMPLATES_A)

    prefixes = ["", "ğŸ”” ", "â€»", "âœ¨ ", ""]
    suffixes = ["", " #ãŠçŸ¥ã‚‰ã›", " #è¡Œæ”¿æ›¸å£«", ""]

    text = f"{random.choice(prefixes)}{template.format(title=title, url=url)}{random.choice(suffixes)}"

    if len(text) > 270:
        text = text[:267] + "..."
    return text

# æŠ•ç¨¿å‡¦ç† v2 API ç”¨
def post_to_twitter(text):
    logging.info("Posting to Twitter (v2 API): %s", text[:80].replace("\n"," ") + ("..." if len(text)>80 else ""))
    try:
        response = client.create_tweet(text=text)
        logging.info("Posted successfully (v2 API). Tweet ID: %s", response.data['id'])
    except Exception as e:
        logging.error("Failed to post (v2 API): %s", e)

# è¨˜äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
def fetch_article_list(base_url):
    logging.info("Fetching site: %s", base_url)
    try:
        r = requests.get(base_url, timeout=15)
        r.raise_for_status()
    except Exception as e:
        logging.error("Failed to fetch site: %s", e)
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    candidates = []

    for art in soup.find_all("article"):
        a = art.find("a", href=True)
        if a:
            title = (a.get_text() or "").strip()
            link = a["href"]
            candidates.append((title, link))

    if not candidates:
        selectors = ["h2 a", ".post a", ".entry-title a", ".article a", ".post-title a"]
        for sel in selectors:
            for a in soup.select(sel):
                if a and a.get("href"):
                    title = (a.get_text() or "").strip()
                    link = a["href"]
                    candidates.append((title, link))

    if not candidates:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith("/") or base_url in href:
                text = (a.get_text() or "").strip()
                if len(text) > 10 and len(text) < 200:
                    link = href if href.startswith("http") else base_url.rstrip("/") + href
                    candidates.append((text, link))

    normalized = []
    for title, link in candidates:
        if not link.startswith("http"):
            link = base_url.rstrip("/") + "/" + link.lstrip("/")
        normalized.append((title, link))

    seen = set()
    out = []
    for t, l in normalized:
        if l not in seen:
            seen.add(l)
            out.append((t, l))
    logging.info("Found %d candidate articles", len(out))
    return out

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    articles = fetch_article_list(TARGET_SITE)
    if not articles:
        logging.error("No articles found; aborting.")
        return

    if random.random() < 0.6:
        chosen = articles[0]
        logging.info("Choosing latest article")
    else:
        chosen = random.choice(articles)
        logging.info("Choosing random past article")

    title, url = chosen
    tweet = compose_text(title, url)

    if os.getenv("TWITTER_DRY_RUN") == "1":
        logging.info("DRY RUN: would post:\n%s", tweet)
    else:
        delay = random.randint(5, 90)
        logging.info("Sleeping %d seconds before posting (human-like delay)", delay)
        time.sleep(delay)
        post_to_twitter(tweet)

if __name__ == "__main__":
    main()
