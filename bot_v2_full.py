# bot_v2_full.pyï¼ˆæ—¥æœ¬èªãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°å¯¾å¿œç‰ˆï¼‰
import os
import random
import time
import logging
import requests
from bs4 import BeautifulSoup
import tweepy
from dotenv import load_dotenv
import re

# -------------------------
# ãƒ­ã‚°è¨­å®š
# -------------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
load_dotenv()

# -------------------------
# APIã‚­ãƒ¼
# -------------------------
API_KEY = os.getenv("API_KEY")
API_SECRET = os.getenv("API_SECRET")
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = os.getenv("ACCESS_TOKEN_SECRET")
BEARER_TOKEN = os.getenv("BEARER_TOKEN")

# -------------------------
# æŠ•ç¨¿å…ˆã‚µã‚¤ãƒˆ
# -------------------------
TARGET_SITE = "https://active-gyoseisyosi.com/"

# -------------------------
# æŠ•ç¨¿ã‚¹ã‚¿ã‚¤ãƒ«
# -------------------------
STYLE_B_WEIGHT = 0.8
TEMPLATES_B = [
    "ã“ã®è¨˜äº‹ã€æ°—ã«ãªã£ãŸæ–¹å¤šã„ã¿ãŸã„ï¼ãœã²èª­ã‚“ã§ã¿ã¦ãã ã•ã„ğŸ‘‡\n{title}\n{url}",
    "æœ€è¿‘ã®ãŠã™ã™ã‚è¨˜äº‹ã§ã™âœ¨\n{title}\n{url}",
    "ãŠã€é¢ç™½ã„æ›´æ–°ãŒã‚ã‚Šã¾ã—ãŸï¼ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ğŸ‘‡\n{title}\n{url}",
]
TEMPLATES_A = [
    "æ–°ã—ã„è¨˜äº‹ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ã”èˆˆå‘³ã‚ã‚Œã°ã”è¦§ãã ã•ã„ã€‚\n{title}\n{url}",
    "æœ€æ–°ã®æŠ•ç¨¿ã‚’ãŠçŸ¥ã‚‰ã›ã—ã¾ã™ã€‚\n{title}\n{url}"
]

# -------------------------
# v2 APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ•ç¨¿ç”¨ï¼‰
# -------------------------
client_v2 = tweepy.Client(
    consumer_key=API_KEY,
    consumer_secret=API_SECRET,
    access_token=ACCESS_TOKEN,
    access_token_secret=ACCESS_TOKEN_SECRET,
    bearer_token=BEARER_TOKEN
)

# -------------------------
# v1.1 APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰
# -------------------------
auth_v1 = tweepy.OAuth1UserHandler(API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
api_v1 = tweepy.API(auth_v1)

# -------------------------
# æ—¥æœ¬èªãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ç”Ÿæˆ
# -------------------------
def make_hashtags(title, max_tags=3):
    # è¨˜å·ã‚„ç©ºç™½ã‚’é™¤å»
    clean = re.sub(r"[^\w\u4e00-\u9fff]", "", title)
    # é•·ã•10æ–‡å­—ä»¥ä¸Šãªã‚‰åˆ‡ã‚‹
    if len(clean) > 20:
        clean = clean[:20]
    return " ".join([f"#{clean}"])

# -------------------------
# æŠ•ç¨¿æ–‡ç”Ÿæˆï¼ˆç›´æ¥ãƒªãƒ³ã‚¯å¯¾å¿œï¼‰
# -------------------------
def compose_text(title, url):
    template = random.choice(TEMPLATES_B) if random.random() < STYLE_B_WEIGHT else random.choice(TEMPLATES_A)

    hashtags = make_hashtags(title)
    prefixes = ["", "ğŸ”” ", "â€»", "âœ¨ ", ""]
    
    # ç›´æ¥ãƒªãƒ³ã‚¯ã‚’ä½¿ã†
    text = f"{random.choice(prefixes)}{template.format(title=title, url=url)} {hashtags}"

    if len(text) > 270:
        text = text[:267] + "..."
    return text

# -------------------------
# fetch_article_list ã®éƒ¨åˆ†ã§ç›¸å¯¾ URL ã¯çµ¶å¯¾ URL ã«å¤‰æ›æ¸ˆã¿
# -------------------------
# ä¾‹ï¼š
# if not link.startswith("http"):
#     link = base_url.rstrip("/") + "/" + link.lstrip("/"

# -------------------------
# æŠ•ç¨¿å‡¦ç†ï¼ˆv2 + v1.1 APIä½µç”¨ï¼‰
# -------------------------
def post_to_twitter(text, image_url=None):
    logging.info("Posting to Twitter: %s", text[:80].replace("\n"," ") + ("..." if len(text)>80 else ""))
    media_ids = None
    try:
        if image_url:
            r = requests.get(image_url, timeout=15)
            r.raise_for_status()
            tmp_path = "/tmp/temp_image.jpg"
            with open(tmp_path, "wb") as f:
                f.write(r.content)
            media = api_v1.media_upload(tmp_path)
            media_ids = [media.media_id]

        response = client_v2.create_tweet(text=text, media_ids=media_ids)
        logging.info("Posted successfully. Tweet ID: %s", response.data['id'])
    except Exception as e:
        logging.error("Failed to post: %s", e)

# -------------------------
# è¨˜äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# -------------------------
def fetch_article_list(base_url):
    logging.info("Fetching site: %s", base_url)
    try:
        r = requests.get(base_url, timeout=15)
        r.raise_for_status()
    except Exception as e:
        logging.error("Failed to fetch site: %s", e)
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    candidates = []
    for art in soup.find_all("article"):
        a = art.find("a", href=True)
        if a:
            title = (a.get_text() or "").strip()
            link = a["href"]
            img = art.find("img")
            img_url = img["src"] if img else None
            candidates.append((title, link, img_url))
    return candidates

# -------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -------------------------
def main():
    articles = fetch_article_list(TARGET_SITE)
    if not articles:
        logging.error("No articles found; aborting.")
        return

    # æœ€æ–°è¨˜äº‹å„ªå…ˆ 60% / éå»è¨˜äº‹æ˜ã‚Šèµ·ã“ã— 40%
    if random.random() < 0.6:
        chosen = articles[0]
        logging.info("Choosing latest article")
    else:
        chosen = random.choice(articles)
        logging.info("Choosing random past article")

    title, url, img_url = chosen
    tweet = compose_text(title, url)

    if os.getenv("TWITTER_DRY_RUN") == "1":
        logging.info("DRY RUN: would post:\n%s", tweet)
    else:
        delay = random.randint(5, 90)
        logging.info("Sleeping %d seconds before posting (human-like delay)", delay)
        time.sleep(delay)
        post_to_twitter(tweet, image_url=img_url)

if __name__ == "__main__":
    main()
